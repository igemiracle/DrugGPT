{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f49a832-ffb7-4bc7-a1f1-ad0aace8c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "sys.path.append('../src')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e2405d-a853-4464-b172-372af9d7c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ocean/projects/bio200049p/yzheng9/conda_envs/llm/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c38e8a1-405a-4ee5-8c49-932753a288a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "598719d9-3175-4622-a8b7-a46462ddee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 16:43:17,127 - INFO - Loading emilyalsentzer/Bio_ClinicalBERT on cuda\n",
      "2024-11-28 16:43:24,983 - INFO - Loading faiss with AVX512 support.\n",
      "2024-11-28 16:43:26,902 - INFO - Successfully loaded faiss with AVX512 support.\n",
      "2024-11-28 16:43:27,156 - INFO - Using CPU for FAISS\n",
      "2024-11-28 16:43:27,157 - INFO - Loading embeddings and metadata...\n",
      "2024-11-28 16:43:27,159 - INFO - Processing file 1/142\n",
      "/ocean/projects/bio200049p/yzheng9/Github/DrugGPT/scripts/../src/Retrival.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(file, map_location='cpu')\n",
      "2024-11-28 16:43:27,587 - INFO - Processing file 2/142\n",
      "2024-11-28 16:43:27,802 - INFO - Processing file 3/142\n",
      "2024-11-28 16:43:28,041 - INFO - Processing file 4/142\n",
      "2024-11-28 16:43:28,273 - INFO - Processing file 5/142\n",
      "2024-11-28 16:43:28,502 - INFO - Processing file 6/142\n",
      "2024-11-28 16:43:28,854 - INFO - Processing file 7/142\n",
      "2024-11-28 16:43:29,195 - INFO - Processing file 8/142\n",
      "2024-11-28 16:43:29,447 - INFO - Processing file 9/142\n",
      "2024-11-28 16:43:29,772 - INFO - Processing file 10/142\n",
      "2024-11-28 16:43:30,030 - INFO - Processing file 11/142\n",
      "2024-11-28 16:43:30,272 - INFO - Processing file 12/142\n",
      "2024-11-28 16:43:30,498 - INFO - Processing file 13/142\n",
      "2024-11-28 16:43:30,768 - INFO - Processing file 14/142\n",
      "2024-11-28 16:43:31,080 - INFO - Processing file 15/142\n",
      "2024-11-28 16:43:31,318 - INFO - Processing file 16/142\n",
      "2024-11-28 16:43:31,538 - INFO - Processing file 17/142\n",
      "2024-11-28 16:43:31,860 - INFO - Processing file 18/142\n",
      "2024-11-28 16:43:32,117 - INFO - Processing file 19/142\n",
      "2024-11-28 16:43:32,387 - INFO - Processing file 20/142\n",
      "2024-11-28 16:43:32,633 - INFO - Processing file 21/142\n",
      "2024-11-28 16:43:32,882 - INFO - Processing file 22/142\n",
      "2024-11-28 16:43:33,187 - INFO - Processing file 23/142\n",
      "2024-11-28 16:43:33,425 - INFO - Processing file 24/142\n",
      "2024-11-28 16:43:33,729 - INFO - Processing file 25/142\n",
      "2024-11-28 16:43:33,981 - INFO - Processing file 26/142\n",
      "2024-11-28 16:43:34,207 - INFO - Processing file 27/142\n",
      "2024-11-28 16:43:34,434 - INFO - Processing file 28/142\n",
      "2024-11-28 16:43:34,717 - INFO - Processing file 29/142\n",
      "2024-11-28 16:43:34,953 - INFO - Processing file 30/142\n",
      "2024-11-28 16:43:35,220 - INFO - Processing file 31/142\n",
      "2024-11-28 16:43:35,448 - INFO - Processing file 32/142\n",
      "2024-11-28 16:43:35,712 - INFO - Processing file 33/142\n",
      "2024-11-28 16:43:35,980 - INFO - Processing file 34/142\n",
      "2024-11-28 16:43:36,268 - INFO - Processing file 35/142\n",
      "2024-11-28 16:43:36,464 - INFO - Processing file 36/142\n",
      "2024-11-28 16:43:36,749 - INFO - Processing file 37/142\n",
      "2024-11-28 16:43:36,990 - INFO - Processing file 38/142\n",
      "2024-11-28 16:43:37,257 - INFO - Processing file 39/142\n",
      "2024-11-28 16:43:37,567 - INFO - Processing file 40/142\n",
      "2024-11-28 16:43:37,814 - INFO - Processing file 41/142\n",
      "2024-11-28 16:43:38,052 - INFO - Processing file 42/142\n",
      "2024-11-28 16:43:38,530 - INFO - Processing file 43/142\n",
      "2024-11-28 16:43:38,781 - INFO - Processing file 44/142\n",
      "2024-11-28 16:43:39,095 - INFO - Processing file 45/142\n",
      "2024-11-28 16:43:39,320 - INFO - Processing file 46/142\n",
      "2024-11-28 16:43:39,559 - INFO - Processing file 47/142\n",
      "2024-11-28 16:43:39,768 - INFO - Processing file 48/142\n",
      "2024-11-28 16:43:40,071 - INFO - Processing file 49/142\n",
      "2024-11-28 16:43:40,332 - INFO - Processing file 50/142\n",
      "2024-11-28 16:43:40,548 - INFO - Processing file 51/142\n",
      "2024-11-28 16:43:40,839 - INFO - Processing file 52/142\n",
      "2024-11-28 16:43:41,101 - INFO - Processing file 53/142\n",
      "2024-11-28 16:43:41,373 - INFO - Processing file 54/142\n",
      "2024-11-28 16:43:41,630 - INFO - Processing file 55/142\n",
      "2024-11-28 16:43:41,947 - INFO - Processing file 56/142\n",
      "2024-11-28 16:43:42,212 - INFO - Processing file 57/142\n",
      "2024-11-28 16:43:42,473 - INFO - Processing file 58/142\n",
      "2024-11-28 16:43:42,807 - INFO - Processing file 59/142\n",
      "2024-11-28 16:43:43,029 - INFO - Processing file 60/142\n",
      "2024-11-28 16:43:43,416 - INFO - Processing file 61/142\n",
      "2024-11-28 16:43:43,707 - INFO - Processing file 62/142\n",
      "2024-11-28 16:43:43,990 - INFO - Processing file 63/142\n",
      "2024-11-28 16:43:44,243 - INFO - Processing file 64/142\n",
      "2024-11-28 16:43:44,455 - INFO - Processing file 65/142\n",
      "2024-11-28 16:43:44,703 - INFO - Processing file 66/142\n",
      "2024-11-28 16:43:44,961 - INFO - Processing file 67/142\n",
      "2024-11-28 16:43:45,202 - INFO - Processing file 68/142\n",
      "2024-11-28 16:43:45,411 - INFO - Processing file 69/142\n",
      "2024-11-28 16:43:45,743 - INFO - Processing file 70/142\n",
      "2024-11-28 16:43:46,035 - INFO - Processing file 71/142\n",
      "2024-11-28 16:43:46,240 - INFO - Processing file 72/142\n",
      "2024-11-28 16:43:46,451 - INFO - Processing file 73/142\n",
      "2024-11-28 16:43:46,728 - INFO - Processing file 74/142\n",
      "2024-11-28 16:43:47,011 - INFO - Processing file 75/142\n",
      "2024-11-28 16:43:47,251 - INFO - Processing file 76/142\n",
      "2024-11-28 16:43:47,551 - INFO - Processing file 77/142\n",
      "2024-11-28 16:43:47,808 - INFO - Processing file 78/142\n",
      "2024-11-28 16:43:48,198 - INFO - Processing file 79/142\n",
      "2024-11-28 16:43:48,459 - INFO - Processing file 80/142\n",
      "2024-11-28 16:43:48,717 - INFO - Processing file 81/142\n",
      "2024-11-28 16:43:48,980 - INFO - Processing file 82/142\n",
      "2024-11-28 16:43:49,236 - INFO - Processing file 83/142\n",
      "2024-11-28 16:43:49,506 - INFO - Processing file 84/142\n",
      "2024-11-28 16:43:49,785 - INFO - Processing file 85/142\n",
      "2024-11-28 16:43:50,011 - INFO - Processing file 86/142\n",
      "2024-11-28 16:43:50,176 - INFO - Processing file 87/142\n",
      "2024-11-28 16:43:50,413 - INFO - Processing file 88/142\n",
      "2024-11-28 16:43:50,646 - INFO - Processing file 89/142\n",
      "2024-11-28 16:43:50,905 - INFO - Processing file 90/142\n",
      "2024-11-28 16:43:51,218 - INFO - Processing file 91/142\n",
      "2024-11-28 16:43:51,472 - INFO - Processing file 92/142\n",
      "2024-11-28 16:43:51,698 - INFO - Processing file 93/142\n",
      "2024-11-28 16:43:51,998 - INFO - Processing file 94/142\n",
      "2024-11-28 16:43:52,274 - INFO - Processing file 95/142\n",
      "2024-11-28 16:43:52,548 - INFO - Processing file 96/142\n",
      "2024-11-28 16:43:52,821 - INFO - Processing file 97/142\n",
      "2024-11-28 16:43:53,091 - INFO - Processing file 98/142\n",
      "2024-11-28 16:43:53,298 - INFO - Processing file 99/142\n",
      "2024-11-28 16:43:53,541 - INFO - Processing file 100/142\n",
      "2024-11-28 16:43:53,792 - INFO - Processing file 101/142\n",
      "2024-11-28 16:43:54,060 - INFO - Processing file 102/142\n",
      "2024-11-28 16:43:54,335 - INFO - Processing file 103/142\n",
      "2024-11-28 16:43:54,603 - INFO - Processing file 104/142\n",
      "2024-11-28 16:43:54,873 - INFO - Processing file 105/142\n",
      "2024-11-28 16:43:55,379 - INFO - Processing file 106/142\n",
      "2024-11-28 16:43:55,649 - INFO - Processing file 107/142\n",
      "2024-11-28 16:43:55,857 - INFO - Processing file 108/142\n",
      "2024-11-28 16:43:56,115 - INFO - Processing file 109/142\n",
      "2024-11-28 16:43:56,402 - INFO - Processing file 110/142\n",
      "2024-11-28 16:43:56,699 - INFO - Processing file 111/142\n",
      "2024-11-28 16:43:56,991 - INFO - Processing file 112/142\n",
      "2024-11-28 16:43:57,269 - INFO - Processing file 113/142\n",
      "2024-11-28 16:43:57,512 - INFO - Processing file 114/142\n",
      "2024-11-28 16:43:57,744 - INFO - Processing file 115/142\n",
      "2024-11-28 16:43:58,044 - INFO - Processing file 116/142\n",
      "2024-11-28 16:43:58,344 - INFO - Processing file 117/142\n",
      "2024-11-28 16:43:58,587 - INFO - Processing file 118/142\n",
      "2024-11-28 16:43:58,937 - INFO - Processing file 119/142\n",
      "2024-11-28 16:43:59,204 - INFO - Processing file 120/142\n",
      "2024-11-28 16:43:59,456 - INFO - Processing file 121/142\n",
      "2024-11-28 16:43:59,708 - INFO - Processing file 122/142\n",
      "2024-11-28 16:43:59,976 - INFO - Processing file 123/142\n",
      "2024-11-28 16:44:00,286 - INFO - Processing file 124/142\n",
      "2024-11-28 16:44:00,527 - INFO - Processing file 125/142\n",
      "2024-11-28 16:44:00,796 - INFO - Processing file 126/142\n",
      "2024-11-28 16:44:01,053 - INFO - Processing file 127/142\n",
      "2024-11-28 16:44:01,267 - INFO - Processing file 128/142\n",
      "2024-11-28 16:44:01,518 - INFO - Processing file 129/142\n",
      "2024-11-28 16:44:01,737 - INFO - Processing file 130/142\n",
      "2024-11-28 16:44:02,008 - INFO - Processing file 131/142\n",
      "2024-11-28 16:44:02,248 - INFO - Processing file 132/142\n",
      "2024-11-28 16:44:02,635 - INFO - Processing file 133/142\n",
      "2024-11-28 16:44:02,946 - INFO - Processing file 134/142\n",
      "2024-11-28 16:44:03,352 - INFO - Processing file 135/142\n",
      "2024-11-28 16:44:03,590 - INFO - Processing file 136/142\n",
      "2024-11-28 16:44:03,953 - INFO - Processing file 137/142\n",
      "2024-11-28 16:44:04,173 - INFO - Processing file 138/142\n",
      "2024-11-28 16:44:04,395 - INFO - Processing file 139/142\n",
      "2024-11-28 16:44:04,773 - INFO - Processing file 140/142\n",
      "2024-11-28 16:44:05,986 - INFO - Processing file 141/142\n",
      "2024-11-28 16:44:06,318 - INFO - Processing file 142/142\n",
      "2024-11-28 16:44:06,884 - INFO - Combined embeddings shape: (141293, 768)\n",
      "2024-11-28 16:44:07,010 - INFO - Building Flat index...\n",
      "2024-11-28 16:44:07,044 - INFO - Adding vectors to index...\n",
      "2024-11-28 16:44:07,269 - INFO - Successfully built index with 141293 vectors\n"
     ]
    }
   ],
   "source": [
    "# 1. First recreate your embedding processor\n",
    "from Embedding import ClinicalEmbeddingProcessor\n",
    "processor = ClinicalEmbeddingProcessor(\n",
    "    processed_path='../data/processed',\n",
    "    model_name='emilyalsentzer/Bio_ClinicalBERT'\n",
    ")\n",
    "\n",
    "# 2. Recreate base retrieval system\n",
    "from Retrival import ClinicalRetrievalSystem\n",
    "retriever = ClinicalRetrievalSystem(\n",
    "    processed_path='../data/processed',\n",
    "    index_type='Flat',\n",
    "    use_gpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60c2af2a-b515-4602-abf4-ebec6268e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Recreate enhanced retrieval system\n",
    "from EnhancedRetrival import EnhancedClinicalRetrieval\n",
    "enhanced_retriever = EnhancedClinicalRetrieval(\n",
    "    base_retriever=retriever,\n",
    "    icd9_map_path='../data/icd9_map.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b31b274e-1706-40fa-996b-f9f5e31b1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "class EnhancedClinicalRetrieval:\n",
    "    def __init__(self, base_retriever, icd9_map_path: str = None):\n",
    "        \"\"\"\n",
    "        Enhanced retrieval system with additional features\n",
    "        \n",
    "        Args:\n",
    "            base_retriever: Base retrieval system instance\n",
    "            icd9_map_path: Path to ICD-9 code mapping file\n",
    "        \"\"\"\n",
    "        self.retriever = base_retriever\n",
    "        self._load_icd9_map(icd9_map_path)\n",
    "        self.setup_logging()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _load_icd9_map(self, icd9_map_path):\n",
    "        \"\"\"Load ICD-9 code mapping\"\"\"\n",
    "        if icd9_map_path and Path(icd9_map_path).exists():\n",
    "            self.icd9_map = pd.read_csv(icd9_map_path)\n",
    "        else:\n",
    "            self.logger.warning(\"ICD-9 mapping file not found\")\n",
    "            self.icd9_map = None\n",
    "    \n",
    "    def _normalize_similarity(self, similarity: float) -> float:\n",
    "\n",
    "        # return (similarity + 1) / 2\n",
    "        # sigmoid \n",
    "        return 1 / (1 + np.exp(-similarity/100))\n",
    "\n",
    "    \n",
    "    def _convert_icd9_codes(self, codes: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convert ICD-9 codes to readable diagnoses using SHORT_TITLE\n",
    "        \n",
    "        Args:\n",
    "            codes: List of ICD-9 codes\n",
    "            \n",
    "        Returns:\n",
    "            List of readable diagnosis strings\n",
    "        \"\"\"\n",
    "        if self.icd9_map is None:\n",
    "            return codes\n",
    "        \n",
    "        readable_codes = []\n",
    "        for code in codes:\n",
    "            code = code.strip()\n",
    "            match = self.icd9_map[self.icd9_map['ICD9_CODE'] == code]\n",
    "            if not match.empty:\n",
    "                # Use SHORT_TITLE for brief description\n",
    "                readable_codes.append(f\"{code}: {match.iloc[0]['SHORT_TITLE']}\")\n",
    "            else:\n",
    "                readable_codes.append(code)\n",
    "        return readable_codes\n",
    "    \n",
    "\n",
    "    \n",
    "    def find_similar_cases(self, \n",
    "                          query_embedding: np.ndarray, \n",
    "                          k: int = 5, \n",
    "                          remove_duplicates: bool = True,\n",
    "                          min_similarity: float = 0.5) -> List[Dict]:\n",
    "\n",
    "        # Get initial results\n",
    "        initial_results = self.retriever.find_similar_cases(\n",
    "            query_embedding, \n",
    "            k=k*2 if remove_duplicates else k\n",
    "        )\n",
    "        \n",
    "        processed_results = []\n",
    "        seen_cases = set()\n",
    "        \n",
    "        for result in initial_results:\n",
    "            try:\n",
    "                # Normalize similarity score\n",
    "                similarity = self._normalize_similarity(result['similarity'])\n",
    "                if similarity < min_similarity:\n",
    "                    continue\n",
    "\n",
    " \n",
    "                # Parse case text\n",
    "                case_data = self._parse_case_text(result['text'])\n",
    "                case_id = result['case_id']\n",
    "                \n",
    "                # Check for duplicates\n",
    "                if remove_duplicates and case_id in seen_cases:\n",
    "                    continue\n",
    "                seen_cases.add(case_id)\n",
    "                \n",
    "                # Convert ICD-9 codes if available\n",
    "                diagnoses = case_data.get('diagnoses', [])\n",
    "                if diagnoses:\n",
    "                    case_data['diagnoses'] = self._convert_icd9_codes(diagnoses)\n",
    "                \n",
    "                # Create enhanced result object\n",
    "                enhanced_result = {\n",
    "                    'case_id': case_id,\n",
    "                    'similarity': similarity,\n",
    "                    'demographics': case_data.get('demographics', ''),\n",
    "                    'diagnoses': case_data.get('diagnoses', []),\n",
    "                    'medications': case_data.get('medications', ''),\n",
    "                    'original_text': result['text']\n",
    "                }\n",
    "                \n",
    "                processed_results.append(enhanced_result)\n",
    "                \n",
    "                if len(processed_results) >= k:\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error processing result: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return processed_results\n",
    " \n",
    "\n",
    "        \n",
    "    def generate_summary(self, results: List[Dict]) -> str:\n",
    "        \"\"\"Generate concise clinical summary\"\"\"\n",
    "        # Count patterns\n",
    "        demographics = defaultdict(int)\n",
    "        diagnoses = defaultdict(int)\n",
    "        medications = defaultdict(int)\n",
    "        \n",
    "        for result in results:\n",
    "            demographics[result['demographics']] += 1\n",
    "            for diag in result['diagnoses']:\n",
    "                diagnoses[diag] += 1\n",
    "            for med in result['medications'].split(','):\n",
    "                if med.strip():\n",
    "                    medications[med.strip()] += 1\n",
    "        \n",
    "        # Format summary\n",
    "        summary = [\n",
    "            f\"Summary of {len(results)} Similar Cases\",\n",
    "            \"-\" * 30,\n",
    "            \n",
    "            \"\\nDemographics:\",\n",
    "            *[f\"- {demo}: {count} cases\" \n",
    "              for demo, count in sorted(demographics.items(), \n",
    "                                      key=lambda x: x[1], reverse=True)],\n",
    "            \n",
    "            \"\\nCommon Diagnoses:\",\n",
    "            *[f\"- {diag}: {count} cases\" \n",
    "              for diag, count in sorted(diagnoses.items(), \n",
    "                                      key=lambda x: x[1], reverse=True)[:5]],\n",
    "            \n",
    "            \"\\nPrescribed Medications:\",\n",
    "            *[f\"- {med}: {count} cases\" \n",
    "              for med, count in sorted(medications.items(), \n",
    "                                     key=lambda x: x[1], reverse=True)[:5]]\n",
    "        ]\n",
    "        \n",
    "        return \"\\n\".join(summary)\n",
    "\n",
    "    def _parse_case_text(self, text: str) -> Dict:\n",
    "        \"\"\"Parse case text into structured data\"\"\"\n",
    "        sections = text.split('[SEP]')\n",
    "        case_data = {}\n",
    "        \n",
    "        for section in sections:\n",
    "            section = section.strip()\n",
    "            if 'Patient:' in section:\n",
    "                case_data['demographics'] = section.replace('Patient:', '').strip()\n",
    "            elif 'Diagnoses:' in section:\n",
    "                codes = section.replace('Diagnoses:', '').strip().split(',')\n",
    "                case_data['diagnoses'] = [code.strip() for code in codes]\n",
    "            elif 'Medications:' in section:\n",
    "                case_data['medications'] = section.replace('Medications:', '').strip()\n",
    "        \n",
    "        return case_data\n",
    "\n",
    "\n",
    "\n",
    "    def analyze_similar_cases(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze patterns in similar cases\"\"\"\n",
    "        analysis = {\n",
    "            'demographics': defaultdict(int),\n",
    "            'diagnoses': defaultdict(int),\n",
    "            'medications': defaultdict(int),\n",
    "            'similarity_stats': {\n",
    "                'mean': np.mean([r['similarity'] for r in results]),\n",
    "                'std': np.std([r['similarity'] for r in results]),\n",
    "                'min': min([r['similarity'] for r in results]),\n",
    "                'max': max([r['similarity'] for r in results])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Count patterns\n",
    "        for result in results:\n",
    "            # Demographics\n",
    "            analysis['demographics'][result['demographics']] += 1\n",
    "            \n",
    "            # Diagnoses\n",
    "            for diagnosis in result['diagnoses']:\n",
    "                analysis['diagnoses'][diagnosis] += 1\n",
    "            \n",
    "            # Medications\n",
    "            meds = result['medications'].split(',')\n",
    "            for med in meds:\n",
    "                if med.strip():\n",
    "                    analysis['medications'][med.strip()] += 1\n",
    "        \n",
    "        # Convert to regular dict and sort\n",
    "        for key in ['demographics', 'diagnoses', 'medications']:\n",
    "            analysis[key] = dict(sorted(\n",
    "                analysis[key].items(), \n",
    "                key=lambda x: x[1], \n",
    "                reverse=True\n",
    "            ))\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def generate_summary(self, results: List[Dict]) -> str:\n",
    "        \"\"\"Generate summary of similar cases analysis\"\"\"\n",
    "        analysis = self.analyze_similar_cases(results)\n",
    "        \n",
    "        summary = [\n",
    "            \"Similar Cases Analysis Summary\",\n",
    "            \"=\" * 30,\n",
    "            f\"\\nFound {len(results)} similar cases\",\n",
    "            f\"Average similarity: {analysis['similarity_stats']['mean']:.3f}\",\n",
    "            \n",
    "            \"\\nCommon Demographics:\",\n",
    "            \"-\" * 20\n",
    "        ]\n",
    "        \n",
    "        for demo, count in list(analysis['demographics'].items())[:3]:\n",
    "            summary.append(f\"- {demo}: {count} cases\")\n",
    "        \n",
    "        summary.extend([\n",
    "            \"\\nTop Diagnoses:\",\n",
    "            \"-\" * 20\n",
    "        ])\n",
    "        \n",
    "        for diag, count in list(analysis['diagnoses'].items())[:5]:\n",
    "            summary.append(f\"- {diag}: {count} cases\")\n",
    "        \n",
    "        summary.extend([\n",
    "            \"\\nCommonly Prescribed Medications:\",\n",
    "            \"-\" * 20\n",
    "        ])\n",
    "        \n",
    "        for med, count in list(analysis['medications'].items())[:5]:\n",
    "            summary.append(f\"- {med}: {count} cases\")\n",
    "        \n",
    "        return \"\\n\".join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1d345-9664-4010-b3ad-983de9f1bb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a41468ab-8d93-4b4b-bf51-d46d0bdba77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional\n",
    "from transformers import AutoConfig\n",
    "\n",
    "@dataclass\n",
    "class RecommendationResult:\n",
    "    recommendation: str\n",
    "    similar_cases: List[Dict]\n",
    "    confidence: float\n",
    "    evidence: str\n",
    "    risks: List[str]\n",
    "\n",
    "\n",
    "class IntegratedMedicalRAG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model_name: str = 'meta-llama/Llama-3.2-1B',\n",
    "        device: str = 'cuda',\n",
    "        retriever: Optional[EnhancedClinicalRetrieval] = None,\n",
    "        processor: Optional[ClinicalEmbeddingProcessor] = None,\n",
    "        min_similarity: float = 0.5,\n",
    "        top_k: int = 5\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.min_similarity = min_similarity\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # First load the tokenizer and add special tokens\n",
    "        logging.info(f\"Loading {llm_model_name}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "        \n",
    "        # Add special tokens if they don't exist\n",
    "        special_tokens = {\n",
    "            'pad_token': '[PAD]',\n",
    "            'eos_token': '</s>',  # Common end of sequence token\n",
    "            'bos_token': '<s>',   # Beginning of sequence token\n",
    "        }\n",
    "        \n",
    "        num_added_tokens = self.tokenizer.add_special_tokens(special_tokens)\n",
    "        \n",
    "        # Load the model with ignore_mismatched_sizes\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(device)\n",
    "        \n",
    "        # Resize embeddings if tokens were added\n",
    "        if num_added_tokens > 0:\n",
    "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        # Update model configuration\n",
    "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.model.config.bos_token_id = self.tokenizer.bos_token_id\n",
    "        self.model.config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        # Set generation config\n",
    "        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.model.generation_config.bos_token_id = self.tokenizer.bos_token_id\n",
    "        self.model.generation_config.eos_token_id = self.tokenizer.eos_token_id\n",
    "        \n",
    "        self.retriever = retriever\n",
    "        self.processor = processor\n",
    "\n",
    "\n",
    "    def _prepare_case_text(self, case_data: Dict) -> str:\n",
    "        \"\"\"Prepare case text for embedding.\"\"\"\n",
    "        case_text = f\"Patient: {case_data.get('age_group', '')} {case_data.get('gender', '')}\"\n",
    "        if 'diagnoses' in case_data:\n",
    "            case_text += f\" [SEP]Diagnoses: {', '.join(case_data['diagnoses'])}\"\n",
    "        if 'medications' in case_data:\n",
    "            case_text += f\" [SEP]Medications: {case_data['medications']}\"\n",
    "        if 'sections' in case_data and 'history' in case_data['sections']:\n",
    "            case_text += f\" [SEP]History: {case_data['sections']['history']}\"\n",
    "        return case_text\n",
    "\n",
    "    def _generate_case_summary(self, similar_cases: List[Dict], current_case: Dict) -> str:\n",
    "        \"\"\"Generate a summary of similar cases and current case context.\"\"\"\n",
    "        summary_parts = []\n",
    "        \n",
    "        # Summarize current case\n",
    "        summary_parts.append(\"Current Case Overview:\")\n",
    "        summary_parts.append(f\"- Patient Demographics: {current_case.get('age_group', 'Unknown')} {current_case.get('gender', 'Unknown')}\")\n",
    "        summary_parts.append(f\"- Current Diagnoses: {', '.join(current_case.get('diagnoses', []))}\")\n",
    "        summary_parts.append(f\"- Current Medications: {current_case.get('medications', 'None')}\")\n",
    "        \n",
    "        # Summarize similar cases\n",
    "        if similar_cases:\n",
    "            summary_parts.append(\"\\nRelevant Case Patterns:\")\n",
    "            diagnoses_counts = {}\n",
    "            medication_patterns = {}\n",
    "            \n",
    "            for case in similar_cases:\n",
    "                # Count diagnoses\n",
    "                for diagnosis in case.get('diagnoses', []):\n",
    "                    diagnoses_counts[diagnosis] = diagnoses_counts.get(diagnosis, 0) + 1\n",
    "                \n",
    "                # Track medication patterns\n",
    "                if 'medications' in case:\n",
    "                    meds = case['medications'].split(',')\n",
    "                    for med in meds:\n",
    "                        med = med.strip()\n",
    "                        medication_patterns[med] = medication_patterns.get(med, 0) + 1\n",
    "            \n",
    "            # Add common diagnoses\n",
    "            common_diagnoses = sorted(diagnoses_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            if common_diagnoses:\n",
    "                summary_parts.append(\"Common Diagnoses in Similar Cases:\")\n",
    "                for diagnosis, count in common_diagnoses:\n",
    "                    summary_parts.append(f\"- {diagnosis}: {count} cases\")\n",
    "            \n",
    "            # Add common medications\n",
    "            common_meds = sorted(medication_patterns.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            if common_meds:\n",
    "                summary_parts.append(\"\\nCommon Medications in Similar Cases:\")\n",
    "                for med, count in common_meds:\n",
    "                    summary_parts.append(f\"- {med}: {count} cases\")\n",
    "        \n",
    "        return \"\\n\".join(summary_parts)\n",
    "\n",
    "    def process_case(self, case_data: Dict) -> RecommendationResult:\n",
    "        \"\"\"Process a single case through the RAG pipeline.\"\"\"\n",
    "        # Get similar cases\n",
    "        case_text = self._prepare_case_text(case_data)\n",
    "        case_embedding = self.processor.get_case_embedding(case_text)\n",
    "        similar_cases = self.retriever.find_similar_cases(\n",
    "            case_embedding,\n",
    "            k=self.top_k,\n",
    "            remove_duplicates=True\n",
    "        )\n",
    "        \n",
    "        # Filter similar cases by relevance\n",
    "        filtered_cases = self._filter_relevant_cases(similar_cases, case_data.get('diagnoses', []))\n",
    "        \n",
    "        # Generate case summary\n",
    "        case_summary = self._generate_case_summary(filtered_cases, case_data)\n",
    "        \n",
    "        # Generate recommendation\n",
    "        prompt = self._construct_prompt(case_data, filtered_cases, case_summary)\n",
    "        recommendation = self._generate_recommendation(prompt)\n",
    "        \n",
    "        # Parse and validate response\n",
    "        result = self._parse_response(recommendation, filtered_cases)\n",
    "        result = self._validate_recommendation(result, case_data)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # ... (保持其他方法不变) ...\n",
    "\n",
    "    def _filter_relevant_cases(self, cases: List[Dict], diagnoses: List[str]) -> List[Dict]:\n",
    "        \"\"\"Filter cases to keep only those most relevant to current diagnoses.\"\"\"\n",
    "        relevant_cases = []\n",
    "        for case in cases:\n",
    "            case_diagnoses = case.get('diagnoses', [])\n",
    "            if any(diag in case_diagnoses for diag in diagnoses):\n",
    "                relevant_cases.append(case)\n",
    "        return relevant_cases[:self.top_k]\n",
    "\n",
    "    def _validate_recommendation(self, result: RecommendationResult, case_data: Dict) -> RecommendationResult:\n",
    "        \"\"\"Validate and enhance recommendation quality.\"\"\"\n",
    "        required_sections = ['Treatment Plan', 'Clinical Rationale', 'Monitoring Plan']\n",
    "        \n",
    "        if not all(section in result.recommendation for section in required_sections):\n",
    "            logging.warning(\"Generated recommendation missing required sections\")\n",
    "            result.confidence *= 0.8\n",
    "        \n",
    "        if not any(med in result.recommendation.lower() for med in case_data.get('medications', '').lower().split(',')):\n",
    "            logging.warning(\"Generated recommendation doesn't reference current medications\")\n",
    "            result.confidence *= 0.9\n",
    "            \n",
    "        return result\n",
    "\n",
    "            \n",
    "    def _construct_prompt(self, case_data: Dict, similar_cases: List[Dict], case_summary: str) -> str:\n",
    "        \"\"\"Construct a more focused and structured prompt.\"\"\"\n",
    "        return f\"\"\"You are an experienced medical professional providing treatment recommendations. Review the following patient case:\n",
    "\n",
    "Patient Profile:\n",
    "- Age Group: {case_data.get('age_group', 'Unknown')}\n",
    "- Gender: {case_data.get('gender', 'Unknown')}\n",
    "- Primary Diagnoses: {', '.join(case_data.get('diagnoses', []))}\n",
    "- Current Medications: {case_data.get('medications', 'None')}\n",
    "- Clinical History: {case_data.get('sections', {}).get('history', 'Not available')}\n",
    "\n",
    "Based on this information, provide a detailed clinical recommendation using the following structure:\n",
    "\n",
    "1. Treatment Plan:\n",
    "- Current medication adjustments (if needed)\n",
    "- New medication recommendations\n",
    "- Specific dosing instructions\n",
    "- Lifestyle modifications\n",
    "\n",
    "2. Clinical Rationale:\n",
    "- Justification for each recommendation\n",
    "- Expected benefits\n",
    "- Treatment goals\n",
    "\n",
    "3. Monitoring Plan:\n",
    "- Specific parameters to monitor\n",
    "- Frequency of monitoring\n",
    "- Target values\n",
    "- Warning signs to watch for\n",
    "\n",
    "Keep your response focused, evidence-based, and clinically precise. Include specific medications, dosages, and monitoring parameters.\"\"\"\n",
    "    \n",
    "    def _generate_recommendation(self, prompt: str) -> str:\n",
    "        \"\"\"Generate recommendation with proper padding configuration.\"\"\"\n",
    "        # Add BOS token at the start if needed\n",
    "        if not prompt.startswith(self.tokenizer.bos_token):\n",
    "            prompt = self.tokenizer.bos_token + \" \" + prompt\n",
    "            \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=1024,\n",
    "                    min_length=200,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.92,\n",
    "                    repetition_penalty=1.3,\n",
    "                    length_penalty=1.0,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    bos_token_id=self.tokenizer.bos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    do_sample=True,\n",
    "                    num_return_sequences=1,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                \n",
    "                return self.tokenizer.decode(\n",
    "                    outputs[0], \n",
    "                    skip_special_tokens=True,\n",
    "                    clean_up_tokenization_spaces=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during generation: {str(e)}\")\n",
    "                return f\"Error generating response: {str(e)}\"\n",
    "                \n",
    "    def _parse_response(self, response: str, similar_cases: List[Dict]) -> RecommendationResult:\n",
    "        \"\"\"Parse response with improved handling.\"\"\"\n",
    "        sections = {\n",
    "            'treatment': '',\n",
    "            'evidence': '',\n",
    "            'risks': []\n",
    "        }\n",
    "        \n",
    "        # More robust section parsing\n",
    "        current_section = None\n",
    "        lines = response.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            if '1. Treatment Plan:' in line or 'Treatment Plan:' in line:\n",
    "                current_section = 'treatment'\n",
    "                continue\n",
    "            elif '2. Clinical Rationale:' in line or 'Clinical Rationale:' in line:\n",
    "                current_section = 'evidence'\n",
    "                continue\n",
    "            elif '3. Risks & Monitoring:' in line or 'Risks:' in line:\n",
    "                current_section = 'risks'\n",
    "                continue\n",
    "                \n",
    "            if current_section == 'treatment' and line.strip():\n",
    "                sections['treatment'] += line.strip() + '\\n'\n",
    "            elif current_section == 'evidence' and line.strip():\n",
    "                sections['evidence'] += line.strip() + '\\n'\n",
    "            elif current_section == 'risks' and line.strip():\n",
    "                if line.startswith('-') or line.startswith('•'):\n",
    "                    sections['risks'].append(line.strip().lstrip('-•').strip())\n",
    "                else:\n",
    "                    sections['risks'].append(line.strip())\n",
    "        \n",
    "        # Clean up sections\n",
    "        sections['treatment'] = sections['treatment'].strip()\n",
    "        sections['evidence'] = sections['evidence'].strip()\n",
    "        sections['risks'] = [risk for risk in sections['risks'] if risk]\n",
    "        \n",
    "        # If sections are empty, try alternate parsing\n",
    "        if not any(sections.values()):\n",
    "            parts = response.split('\\n\\n')\n",
    "            if len(parts) >= 3:\n",
    "                sections['treatment'] = parts[0]\n",
    "                sections['evidence'] = parts[1]\n",
    "                sections['risks'] = [r.strip() for r in parts[2].split('\\n') if r.strip()]\n",
    "        \n",
    "        # Calculate confidence based on content\n",
    "        confidence = self._calculate_confidence(sections, similar_cases)\n",
    "        \n",
    "        return RecommendationResult(\n",
    "            recommendation=sections['treatment'],\n",
    "            similar_cases=similar_cases,\n",
    "            confidence=confidence,\n",
    "            evidence=sections['evidence'],\n",
    "            risks=sections['risks']\n",
    "        )\n",
    "    \n",
    "    def _calculate_confidence(self, sections: Dict, similar_cases: List[Dict]) -> float:\n",
    "        \"\"\"Calculate confidence score based on response quality and similar cases.\"\"\"\n",
    "        base_confidence = 0.5\n",
    "        \n",
    "        # Check response completeness\n",
    "        if sections['treatment'] and sections['evidence'] and sections['risks']:\n",
    "            base_confidence += 0.2\n",
    "        \n",
    "        # Check for specific medical content\n",
    "        medical_indicators = ['mg', 'dose', 'monitor', 'adjust', 'increase', 'decrease']\n",
    "        content = sections['treatment'].lower()\n",
    "        medical_term_count = sum(1 for term in medical_indicators if term in content)\n",
    "        base_confidence += min(0.2, medical_term_count * 0.03)\n",
    "        \n",
    "        # Factor in similar cases similarity scores\n",
    "        if similar_cases:\n",
    "            avg_similarity = np.mean([case['similarity'] for case in similar_cases])\n",
    "            base_confidence *= (0.5 + 0.5 * avg_similarity)  # Weight similarity at 50%\n",
    "        \n",
    "        return min(1.0, base_confidence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b782ab5-d1f7-4850-b013-6788b9a3c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted results\n",
    "print(\"\\n=== Treatment Recommendation ===\")\n",
    "print(result.recommendation)\n",
    "\n",
    "print(\"\\n=== Clinical Rationale ===\")\n",
    "print(result.evidence)\n",
    "\n",
    "print(\"\\n=== Risks and Precautions ===\")\n",
    "for risk in result.risks:\n",
    "    print(f\"• {risk}\")\n",
    "\n",
    "print(f\"\\nConfidence Score: {result.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "26c6a6de-f1ec-4b31-aa45-332e51ffdc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 19:37:20,388 - INFO - Loading meta-llama/Llama-3.2-1B...\n",
      "2024-11-28 19:39:29,127 - WARNING - Generated recommendation missing required sections\n",
      "2024-11-28 19:39:29,129 - WARNING - Generated recommendation doesn't reference current medications\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Treatment Recommendation ===\n",
      "- Current medication adjustments (if needed)\n",
      "- New medication recommendations\n",
      "- Specific dosing instructions\n",
      "- Lifestyle modifications\n",
      "\n",
      "=== Clinical Rationale ===\n",
      "- Justification for each recommendation\n",
      "- Expected benefits\n",
      "- Treatment goals\n",
      "3. Monitoring Plan:\n",
      "- Specific parameters to monitor\n",
      "- Frequency of monitoring\n",
      "- Target values\n",
      "- Warning signs to watch for\n",
      "Keep your response focused, evidence-based, and clinically precise. Include specific medications, dosages, and monitoring parameters. Do not include irrelevant details.\n",
      "This is intended as feedback ONLY; please see our Feedback Policy prior to posting new responses.\n",
      "Thank you!Question from Dr.Lee,\n",
      "You have successfully created two items in this topic but can't delete them?\n",
      "Hi there,\n",
      "I don’t know why I cannot make these change after creating it? Did my post get rejected somehow?\n",
      "Is that possible or should i re-post one by another name?\n",
      "Thanks!\n",
      "Yes,you need click \"Report\" button at top right corner beside answer icon.If u use wrong username/password,it will be blocked.Thanks!\n",
      "A person comes across such site may think its spam,but actually he/she has done mistake.So when user reports like “Spam” we remove all those posts which do match spambots’s profile so users wonot loose any time spent browsing their content.I hope now they wont waste more of his/her valuable time again.Tnx!!\n",
      "It happens because some people reported incorrectly.So what did happened was just removing un-relevant stuff without editing.It means other members cant read through same kind of errors while making comment.But if someone who posted original problem correctly would still benefit others also,then no worries about deleting comments.Sorry!! Hope everything clears.\n",
      "How To Post On The Web - Forums And Discussion Boards -\n",
      "Forums & discussion boards allow participants access to chat rooms where discussions take place among forum visitors regarding various subjects relating to medicine.Hence patients share ideas related issues pertaining thereto.Discussion forums help individuals exchange different opinions based upon personal experiences.Some useful hints provided here might aid readers resolve problems encountered during consultation.To achieve success follow basic guidelines given below before publishing online.For instance-\n",
      "If writing article/essay/review/post/comment/poem etc then write clearly understandable language.Follow appropriate spelling rules.Try avoiding slang words unless medically justified.Therefore refrain from employing abbreviations especially acronyms.Also stick strictly to proper grammar.The purpose behind keeping website clean,is increasing search engine rankings.By doing above things,single word misspelling could become double meaning phrase.Do keep reading carefully.This way anyone encountering error due to lack knowledge doesnot remain clueless.Moreover try avoid excessive punctuation.A good practice follows APA format,i.e. put ‘(a)’ insteadof‘and’,or alternatively append semicolon (;).While adding commas remember never mix up alphabets.Make sure sentences start well spaced apart.When placing multiple paragraphs within text area always align both first line of every paragraph towards left margin.Use bold letters only once per sentence.Never repeat header tags.Here's how headers work;\n",
      "*Subtle differences between headings *\n",
      "Header style A indicates subheadings.Subheads appear underlined.They must fit into column layout perfectly.As mentioned previously,bold font helps highlighting important points.Remember underline appears green color.In addition,the title automatically changes fontsize,size,padding,and even alignment.One thing remains unchanged–boldness.Underline stays black regardless whether heading level increases.Note:- If necessary,in future add additional notes,e.g.,references,references cited list.However,this section requires special attention.Young age group includes children ages six years old to seventeen year olds.And young adults range twelve-to nineteen-years-old.Based off previous data published last month,I feel twenty-five percent probability exists child aged nine falls diabetic.While remaining fifty-two percent population over eighteen becomes affected.After consulting primary care physician,diseases commonly seen pediatricians treat are listed bellow;\n",
      "Acute respiratory infections,Congenital heart disease,Hypothyroidism,Tuberculosis,Dermatologic diseases,Meningitis,Polyneuritic pains,Gastrointestinal disorders,Breast cancer,Renal cysts,Kidney stones,Nephritis,Osteoporosis,Lupus,Sickle cellanemia,Ulcerative colitisis.Chronic conditions asthma,cholera,vaginitis,gonorrhea,epilepsy,hepatitis,malignant tumors,&cancer.Prevention measures included physical fitness,proper nutrition,cleanliness,fresh air,intake water containing vitamin C,kitchen hygiene,aerobic exercise,non-smoking habit,lifestyle modification,wearing safety helmets,tooth\n",
      "\n",
      "=== Risks and Precautions ===\n",
      "\n",
      "Confidence Score: 0.38\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize with the same parameters\n",
    "integrated_rag = IntegratedMedicalRAG(\n",
    "    retriever=enhanced_retriever,\n",
    "    processor=processor,\n",
    "    min_similarity=0.5,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Same test case\n",
    "test_case = {\n",
    "    'age_group': '31-50',\n",
    "    'gender': 'F',\n",
    "    'diagnoses': ['Type 2 Diabetes', 'Hypertension'],\n",
    "    'medications': 'metformin 1000mg BID, lisinopril 10mg daily',\n",
    "    'sections': {\n",
    "        'history': 'Patient with 5-year history of T2DM, recently diagnosed hypertension',\n",
    "        'plan': 'Medication adjustment needed due to suboptimal control'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate new recommendation\n",
    "result = integrated_rag.process_case(test_case)\n",
    "\n",
    "# Print formatted results\n",
    "print(\"\\n=== Treatment Recommendation ===\")\n",
    "print(result.recommendation)\n",
    "\n",
    "print(\"\\n=== Clinical Rationale ===\")\n",
    "print(result.evidence)\n",
    "\n",
    "print(\"\\n=== Risks and Precautions ===\")\n",
    "for risk in result.risks:\n",
    "    print(f\"• {risk}\")\n",
    "\n",
    "print(f\"\\nConfidence Score: {result.confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bdd3daf-93f6-43bc-b0c7-f624f556ad42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing medical generation...\n",
      "\n",
      "Test output: You are a medical professional. A patient presents with Type 2 Diabetes (T2DM) and Hypertension. Current medications: metformin 1000mg BID and lisinopril 10mg daily.\n",
      "\n",
      "Provide specific treatment recommendations in the following format:\n",
      "1. Treatment Plan\n",
      "2. Clinical Rationale\n",
      "3. Risks and Monitoring\n",
      "\n",
      "Your response should be detailed and professional. Begin: \"Type 2 diabetes is characterized by an increased risk of developing cardiovascular disease, renal dysfunction, retinal diseases such as diabetic macular edema or nephropathy.\"\n",
      "\n",
      "Please see my completed assignment for formatting details.\n",
      "Question:\n",
      "\n",
      "Create a complete nursing diagnosis statement that will best address this topic's clinical relevance to your assigned patients' needs/condition(s). Include the relevant evidence-based practice model/s and/or strategies to implement effective interventions identified during our discussion on related literature review. Use correct APA citation style throughout!\n",
      "Clinical Scenario:\n",
      "\n",
      "A female client reports having multiple symptoms that include decreased urine output, fatigue, shortness of breath, dizziness, heart palpitations, and fever at home after returning from vacation overseas. The nurse has been given her permission to administer additional fluids. You need help creating a plan that addresses these symptoms! Provide clear documentation guidelines that include rationale/supporting information you used to develop the plan!\n",
      "\n",
      "This question requires the use of both scientific research and personal experience when answering it. It also asks about using evidence based practices which means we must provide citations. This is not just any old paper; it's real work based upon actual situations so there MUST BE RESEARCH CITED!!!!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "https://studydriver.com/wp-content/uploads/2021/03/logo-300x75.png 0 0 Joseph https://study-driver.com/wp-conten... https://.studydriver.com/studyguide/case-studies/sample-case-report-on-diabetes-treatment-plan-for-a-patient-with-type-2-diabetic-nephropathic-calcification-and-high-blood-pressure-solution/\n",
      "Case Study Report On Diabetics Nephrocalciative High Blood Pressure Solution – Case Study Assignment Help | Professional Essay Writer Service UK\n",
      "\n",
      "\n",
      "#4 - Nursing Process: Diagnosis\n",
      "\n",
      "• Step #1: Assessment\n",
      "Step #2: Analysis • Interventions & Outcomes • Recommendations • Implementation\n",
      "Write each part individually.\n",
      "\n",
      "\n",
      "Diagnosis is defined as identifying problems within individuals who exhibit signs and symptoms associated with illness processes. Nurses can apply various assessment tools depending on their role’s specialty. For example, nurses working as physicians would utilize\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_prompt = \"\"\"You are a medical professional. A patient presents with Type 2 Diabetes (T2DM) and Hypertension. Current medications: metformin 1000mg BID and lisinopril 10mg daily.\n",
    "\n",
    "Provide specific treatment recommendations in the following format:\n",
    "1. Treatment Plan\n",
    "2. Clinical Rationale\n",
    "3. Risks and Monitoring\n",
    "\n",
    "Your response should be detailed and professional. Begin:\"\"\"\n",
    "\n",
    "print(\"Testing medical generation...\")\n",
    "inputs = integrated_rag.tokenizer(test_prompt, return_tensors=\"pt\").to(integrated_rag.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = integrated_rag.model.generate(\n",
    "        **inputs,\n",
    "        max_length=512, \n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=3,\n",
    "        do_sample=True\n",
    "    )\n",
    "test_output = integrated_rag.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\nTest output:\", test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9a3f766-6277-44ac-a8af-209428a8ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar Cases:\n",
      "\n",
      "Similarity: 0.8836029302966946\n",
      "Demographics: 71-89 F\n",
      "Diagnoses: ['0383: Anaerobic septicemia', '00845: Int inf clstrdium dfcile', '99591: Sepsis', '4589: Hypotension NOS', '49390: Asthma NOS', '2809: Iron defic anemia NOS', '6202: Ovarian cyst NEC/NOS', '2189: Uterine leiomyoma NOS']\n",
      "\n",
      "Similarity: 0.8828982690582066\n",
      "Demographics: 71-89 M\n",
      "Diagnoses: ['6820: Cellulitis of face', '0389: Septicemia NOS', '99591: Sepsis', 'E9289: Accident NOS']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = integrated_rag.process_case(test_case)\n",
    "similar_cases = result.similar_cases\n",
    "print(\"\\nSimilar Cases:\")\n",
    "for case in similar_cases[:2]: \n",
    "    print(f\"\\nSimilarity: {case['similarity']}\")\n",
    "    print(f\"Demographics: {case['demographics']}\")\n",
    "    print(f\"Diagnoses: {case['diagnoses']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9dbc2-dabb-45b1-ae38-a181eb2c4da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
